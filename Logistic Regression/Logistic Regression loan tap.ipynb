{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import RocCurveDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('/Users/arfatshaikh/Documents/Machine-Learning/Logistic Regression/dataset/loantap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the setting to display max_columns in dataframe\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking null values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checcking values in each column for categorical variables\n",
    "for i in data.columns:\n",
    "    if data[i].dtype == 'object':\n",
    "        print(i)\n",
    "        print(data[i].unique())\n",
    "        print(\"--------\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['home_ownership'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging ANY and NONE to OTHER becasue they are not significant\n",
    "data['home_ownership'] = np.where(data['home_ownership'].isin(['ANY', 'NONE']), \"OTHER\", data['home_ownership'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['term'].replace({' 36 months': 36, ' 60 months': 60}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique patterns in address field\n",
    "[\n",
    "    '0174 Michelle Gateway\\r\\nMendozaberg, OK 22690',\n",
    "    'USCGC Roth\\r\\nFPO AA 70466',\n",
    "    'Unit 8386 Box 5821\\r\\nDPO AE 05113',\n",
    "    'USNV Trujillo\\r\\nFPO AA 30723',\n",
    "    'PSC 5108, Box 2953\\r\\nAPO AP 05113',\n",
    "    'USS Ramirez\\r\\nFPO AP 29597',\n",
    "    'USNS Roberts\\r\\nFPO AA 11650'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address split function\n",
    "def split_address(addr):\n",
    "    match = re.match(r'(?P<house_no>\\d+)\\s+(?P<first_add>.*?)\\r\\n(?P<second_add>.*?),\\s+(?P<state>[A-Z]+)\\s+(?P<zipcode>\\d+)', addr)\n",
    "    if match:\n",
    "        return pd.Series(match.groupdict())\n",
    "    return pd.Series([None]*5, index=['house_no', 'first_add', 'second_add', 'state', 'zipcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['address'] = data['address'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['address'].head())  # Check the first few rows\n",
    "print(data['address'].dtype)  # Check the data type\n",
    "print(data['address'].isnull().sum())  # Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_split = data['address'].apply(split_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data, address_split], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['emp_title'].isna(), ['emp_title']] = 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['title'].isna(), ['title']] = 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['emp_length'].isna(), ['emp_length']] = 'Unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USCGC Nunez\\r\\nFPO AE 30723,\n",
    "Unit 8386 Box 5821\\r\\nDPO AE 05113,\n",
    "USNV Trujillo\\r\\nFPO AA 30723,\n",
    "PSC 5108, Box 2953\\r\\nAPO AP 05113,\n",
    "USS Goodman\\r\\nFPO AE 22690,\n",
    "USNS Roberts\\r\\nFPO AA 11650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern2(addr):\n",
    "    pattern = (\n",
    "    r'(?P<house_no>[A-Za-z0-9,]+(?:\\s+[A-Za-z0-9,]+)*)'  # house_no: allows letters, numbers, spaces, and commas\n",
    "    r'\\s+(?P<first_add>Box\\s+\\d+)'                       # first_add: Box number (e.g., Box 2953)\n",
    "    r'\\r\\n(?P<second_add>[A-Za-z]{3})'                   # second_add: military type (e.g., APO)\n",
    "    r'\\s+(?P<state>[A-Z]{2})'                            # state code (e.g., AP)\n",
    "    r'\\s+(?P<zipcode>\\d{5})'                             # 5-digit zip code\n",
    ")\n",
    "\n",
    "    # Apply regex to extract the parts\n",
    "    match = re.match(pattern, addr)\n",
    "    if match:\n",
    "        result = match.groupdict()\n",
    "        result['first_add'] = result['first_add'] if result['first_add'] else ''\n",
    "        return pd.Series(result)\n",
    "    return pd.Series([None]*5, index=['house_no', 'first_add','second_add', 'state', 'zipcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern3(addr):\n",
    "    pattern = (\n",
    "    r'(?P<house_no>[A-Za-z0-9]+)'                    # house_no: first word only (e.g., USNV)\n",
    "    r'\\s+(?P<first_add>[^\\r\\n]+)'                    # first_add: until newline (e.g., Trujillo)\n",
    "    r'\\r\\n(?P<second_add>[A-Za-z]{3})'               # second_add: FPO/DPO/APO\n",
    "    r'\\s+(?P<state>[A-Z]{2})\\s+(?P<zipcode>\\d{5})'   # state and 5-digit zip\n",
    ")\n",
    "\n",
    "    # Apply regex to extract the parts\n",
    "    match = re.match(pattern, addr)\n",
    "    if match:\n",
    "        result = match.groupdict()\n",
    "        result['first_add'] = result['first_add'] if result['first_add'] else ''\n",
    "        return pd.Series(result)\n",
    "    return pd.Series([None]*5, index=['house_no', 'first_add','second_add', 'state', 'zipcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['address'].str.split(' ').str[0].isin(['Unit','PSC']),'address'].apply(pattern2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the pattern function to rows where 'house_no' is None and 'address' starts with 'USCGC'\n",
    "uscgc_rows = data.loc[data['address'].str.split(' ').str[0].isin(['Unit','PSC'])]\n",
    "updated_values = uscgc_rows['address'].apply(pattern2)\n",
    "\n",
    "# Update the columns with the extracted values where they are None\n",
    "for col in ['house_no', 'first_add', 'second_add', 'state', 'zipcode']:\n",
    "    data.loc[data['address'].str.split(' ').str[0].isin(['Unit','PSC']), col] = data.loc[data['address'].str.split(' ').str[0].isin(['Unit','PSC']), col].fillna(updated_values[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['address'].str.split(' ').str[0].isin(['USNV','USS','USCGC','USNS']),'address'].apply(pattern3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the pattern function to rows where 'house_no' is None and 'address' starts with 'USCGC'\n",
    "uscgc_rows = data.loc[data['address'].str.split(' ').str[0].isin(['USNV','USS','USCGC','USNS'])]\n",
    "updated_values = uscgc_rows['address'].apply(pattern3)\n",
    "\n",
    "# Update the columns with the extracted values where they are None\n",
    "for col in ['house_no', 'first_add', 'second_add', 'state', 'zipcode']:\n",
    "    data.loc[data['address'].str.split(' ').str[0].isin(['USNV','USS','USCGC','USNS']), col] = data.loc[data['address'].str.split(' ').str[0].isin(['USNV','USS','USCGC','USNS']), col].fillna(updated_values[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['revol_util'].isna(), ['revol_util']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['mort_acc'].isna(), ['mort_acc']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['pub_rec_bankruptcies'].isna(), ['pub_rec_bankruptcies']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('address', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pub_rec'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pub_rec(number):\n",
    "    if number == 0.0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def mort_acc(number):\n",
    "    if number == 0.0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def pub_rec_bankruptcies(number):\n",
    "    if number == 0.0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pub_rec'] = data.pub_rec.apply(pub_rec)\n",
    "data['mort_acc'] = data.mort_acc.apply(mort_acc)\n",
    "data['pub_rec_bankruptcies'] = data.pub_rec_bankruptcies.apply(pub_rec_bankruptcies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title'] = data.title.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['initial_list_status'] = data['initial_list_status'].map({'f': 0, 'w': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier treatement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df, column):\n",
    "    \"\"\"\n",
    "    Detects outliers using the IQR method for a given column.\n",
    "    \n",
    "    Returns:\n",
    "    - mask of outliers (boolean Series)\n",
    "    - percentage of outliers in that column\n",
    "    \"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Boolean mask for outliers\n",
    "    outliers = (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "    outlier_count = outliers.sum()\n",
    "    total_count = df[column].notna().sum()\n",
    "    \n",
    "    outlier_percentage = (outlier_count / total_count) * 100\n",
    "\n",
    "    print(f\"Outliers in '{column}': {outlier_count} out of {total_count} ({outlier_percentage:.2f}%)\")\n",
    "    \n",
    "    return outliers, outlier_percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.columns:\n",
    "    if data[i].dtype == 'int64' or data[i].dtype == 'float64':\n",
    "        outliers, percentage = detect_outliers_iqr(data, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for col in data.columns:\n",
    "#    if data[col].dtype == 'int64' or data[col].dtype == 'float64':\n",
    "#        mean = data[col].mean()\n",
    "#        std = data[col].std()\n",
    "#        upper_limit = mean+3*std\n",
    "#        lower_limit = mean-3*std\n",
    "#        data = data[(data[col]<upper_limit) & (data[col]>lower_limit)]\n",
    "#data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['loan_amnt','installment','annual_inc','revol_bal','revol_util','dti']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clipping the outliers to 99 percentile values\n",
    "data['loan_amnt'] = np.where(data['loan_amnt'] > 35000, 35000, data['loan_amnt'])\n",
    "data['installment'] = np.where(data['installment'] > 1000, 1000, data['installment'])\n",
    "data['annual_inc'] = np.where(data['annual_inc'] > 150000.0, 150000.0, data['annual_inc'])\n",
    "data['revol_bal'] = np.where(data['revol_bal'] > 40000, 40000, data['revol_bal'])\n",
    "data['revol_util'] = np.where(data['revol_util'] > 100, 100, data['revol_util'])\n",
    "data['dti'] = np.where(data['dti'] > 36, 36, data['dti'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['loan_amnt','installment','annual_inc','revol_bal','revol_util','dti']:\n",
    "    if data[i].dtype == 'int64' or data[i].dtype == 'float64':\n",
    "        outliers, percentage = detect_outliers_iqr(data, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['issue_d','earliest_cr_line'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['loan_status'] = data['loan_status'].map({'Fully Paid': 0, 'Charged Off': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes(include=['object']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoding_columns = ['grade','sub_grade','emp_length','title','house_no','first_add','second_add','state','emp_title']\n",
    "one_hot_encoding_columns = ['home_ownership','verification_status','purpose','zipcode','application_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in label_encoding_columns:\n",
    "    if data[i].dtype == 'object' or data[i].dtype == 'datetime64[ns]':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "for i in label_encoding_columns:\n",
    "    if data[i].dtype == 'object' or data[i].dtype == 'datetime64[ns]':\n",
    "        data[i] = le.fit_transform(data[i])\n",
    "        print(i,'Label Encoding Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=one_hot_encoding_columns, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='loan_status', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['loan_status'])\n",
    "Y = data['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = Scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling Minority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res, Y_res = sm.fit_resample(X_scaled, Y)\n",
    "print(X_res.shape)\n",
    "print(Y_res.shape)\n",
    "print(Y_res.value_counts())\n",
    "print(Y_res.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "def calculate_vif(df, features):\n",
    "    \"\"\"\n",
    "    Calculate VIF for a set of features in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - features: list of column names (strings)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with features and their corresponding VIF scores\n",
    "    \"\"\"\n",
    "    X = df[features].copy()\n",
    "    X = add_constant(X)  # Add intercept term for VIF calculation\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    \n",
    "    return vif_data.drop(index=0)  # Drop the constant row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_res and Y_res to a pandas DataFrame\n",
    "X_res_df = pd.DataFrame(X_res, columns=X.columns)\n",
    "Y_res_df = pd.DataFrame(Y_res, columns=['loan_status'])\n",
    "\n",
    "# Combine them into a single DataFrame if needed\n",
    "resampled_data = pd.concat([X_res_df, Y_res_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_vif(resampled_data, resampled_data.columns).sort_values(by='VIF', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_data.drop(columns=['loan_amnt','sub_grade','installment','grade','int_rate','term','purpose_debt_consolidation','purpose_credit_card','purpose_home_improvement','purpose_other'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(resampled_data.drop(columns='loan_status'), resampled_data.loan_status, test_size=0.2, random_state=42)\n",
    "log_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_reg.predict(X_test)\n",
    "y_pred_proba = log_reg.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = log_reg.predict(X_train)\n",
    "y_pred_proba_train = log_reg.predict_proba(X_train)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Accuracy: \", accuracy_score(y_train, y_pred_train))\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_train = confusion_matrix(y_train, y_pred_train)\n",
    "confusion_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "print(\"Train Confusion Matrix:\\n\", confusion_matrix_train)\n",
    "print(\"Test Confusion Matrix:\\n\", confusion_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = confusion_matrix(y_train, y_pred_train)[0][0]\n",
    "TP = confusion_matrix(y_train, y_pred_train)[1][1]\n",
    "FN = confusion_matrix(y_train, y_pred_train)[1][0]\n",
    "FP = confusion_matrix(y_train, y_pred_train)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Precision = TP / (TP + FP)\n",
    "Recall = TP / (TP + FN)\n",
    "print(\"Precision:\", Precision)\n",
    "print(\"Recall:\", Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_test = classification_report(y_test, y_pred)\n",
    "print(\"Test Classification Report:\\n\", classification_report_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_roc_auc = roc_auc_score(y_test, y_pred)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Regularized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_regularized = LogisticRegression(max_iter=10000, random_state=42, C=100, penalty='l2')\n",
    "log_reg_regularized.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Accuracy: \", accuracy_score(y_train, log_reg_regularized.predict(X_train)))\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, log_reg_regularized.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cross_val_scores = cross_val_score(log_reg_regularized, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "print(\"Cross-validation scores: \", cross_val_scores)\n",
    "print(\"Mean cross-validation score: \", cross_val_scores.mean())\n",
    "print(\"Standard Deviation of cross-validation scores: \", cross_val_scores.std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
